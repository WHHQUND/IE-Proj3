{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6816b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import librosa\n",
    "import numpy as np\n",
    "import sys\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataset import AsrDataset\n",
    "from model import LSTM_ASR\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1ccded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AsrDataset(Dataset):\n",
    "    def __init__(self, file_lbls, lbl_names='data/clsp.lblnames', text=None):\n",
    "        \"\"\"\n",
    "        :param scr_file: clsp.trnscr\n",
    "        :param feature_type: \"quantized\" or \"mfcc\"\n",
    "        :param feature_file: clsp.trainlbls or clsp.devlbls\n",
    "        :param feature_label_file: clsp.lblnames\n",
    "        :param wav_scp: clsp.trnwav or clsp.devwav\n",
    "        :param wav_dir: wavforms/\n",
    "        \"\"\"\n",
    "        assert self.feature_type in ['discrete', 'mfcc']\n",
    "\n",
    "        self.blank = \"<blank>\"\n",
    "        self.silence = \"<sil>\"\n",
    "\n",
    "        # === write your code here ===\n",
    "        \n",
    "        # Create a dictionary which store the alphabet\n",
    "        phones = {'_':27,\n",
    "                'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':6, 'g':7,\n",
    "                'h':8, 'i':9, 'j':10, 'k':11, 'l':12, 'm':13, 'n':14, 'o':15, 'p':16,\n",
    "                'q':17, 'r':18, 's':19, 't':20, 'u':21, 'v':22,\n",
    "                'w':23, 'x':24, 'y':25, 'z':26}\n",
    "        phones_rev = {v: k for k, v in phones.items()}\n",
    "        self.phones = phones\n",
    "        self.phones_rev = phones_rev\n",
    "        self.text = text\n",
    "        \n",
    "        # Create vocab and label to index\n",
    "        lblnames = []\n",
    "        with open(lbl_names, 'r') as n:\n",
    "            lines = n.readlines()[1:]\n",
    "            for l in lines:\n",
    "                lblnames.append(l.strip('\\n'))\n",
    "        index = [num for num in range(1,len(lblnames)+1)]\n",
    "        self.vocab = {lblnames[i] : index[i] for i in range(len(lblnames))}\n",
    "        \n",
    "        # Create word_labels, used in train_test_split\n",
    "        self.word_labels = []\n",
    "        self.dataset = self.load_quantized_features(file_lbls, text=text)\n",
    "        self.word_labels = np.array(self.word_labels)\n",
    "        \n",
    "    def load_quantized_features(self, file_lbls, text=None):\n",
    "        dataset = {}\n",
    "\n",
    "        # Extract labels for each utterance and convert to index tensor\n",
    "        lbl_seqs = []\n",
    "        with open(file_lbls, 'r') as t:\n",
    "            lines = t.readlines()[1:]\n",
    "            for j in range(len(lines)):\n",
    "                lbls = lines[j].split(\" \")[:-1]\n",
    "                l_tensor = []\n",
    "                for lbl in lbls:\n",
    "                    l_idx = self.vocab.get(lbl)\n",
    "                    l_tensor.append(l_idx)\n",
    "                lbl_seqs.append(torch.tensor(l_tensor))\n",
    "\n",
    "        # Read in the words and convert to index tensor\n",
    "        if text is not None:\n",
    "            words = []\n",
    "            with open(text, 'r') as s:\n",
    "                lines = s.readlines()[1:]\n",
    "                for l in lines:\n",
    "                    w = '_' + l.strip('\\n') + '_'\n",
    "                    w_tensor = []\n",
    "                    for let in list(w):\n",
    "                        w_idx = self.phones.get(let)\n",
    "                        w_tensor.append(w_idx)\n",
    "                    words.append(torch.tensor(w_tensor))\n",
    "\n",
    "            for idx in range(len(lbl_seqs)):\n",
    "                dataset.update({idx: {'feats':lbl_seqs[idx], 'target_tokens':words[idx]}})\n",
    "                w_lbl = int(''.join(map(str, words[idx].detach().numpy().tolist())))\n",
    "                self.word_labels.append(w_lbl)\n",
    "        else: \n",
    "            for idx in range(len(lbl_seqs)):\n",
    "                dataset.update({idx: {'feats':lbl_seqs[idx]}})\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: num_of_samples\n",
    "        \"\"\"\n",
    "        return len(self.script)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get one sample each time. Do not forget the leading- and trailing-silence.\n",
    "        :param idx: index of sample\n",
    "        :return: spelling_of_word, feature\n",
    "        \"\"\"\n",
    "        # === write your code here ===\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if self.text is None:\n",
    "            return self.dataset[idx]['feats'], None\n",
    "        return self.dataset[idx]['feats'], self.dataset[idx]['target_tokens']\n",
    "\n",
    "\n",
    "    # This function is provided\n",
    "    def compute_mfcc(self, wav_scp, wav_dir):\n",
    "        \"\"\"\n",
    "        Compute MFCC acoustic features (dim=40) for each wav file.\n",
    "        :param wav_scp:\n",
    "        :param wav_dir:\n",
    "        :return: features: List[np.ndarray, ...]\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        with open(wav_scp, 'r') as f:\n",
    "            for wavfile in f:\n",
    "                wavfile = wavfile.strip()\n",
    "                if wavfile == 'jhucsp.trnwav':  # skip header\n",
    "                    continue\n",
    "                wav, sr = librosa.load(os.path.join(wav_dir, wavfile), sr=None)\n",
    "                feats = librosa.feature.mfcc(y=wav, sr=16e3, n_mfcc=40, hop_length=160, win_length=400).transpose()\n",
    "                features.append(feats)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5434f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LSTM_ASR_Discrete(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size=257, alphabet_size=28):\n",
    "        super().__init__()\n",
    "        assert feature_type in ['discrete', 'mfcc']\n",
    "        # Build your own neural network. Play with different hyper-parameters and architectures.\n",
    "        # === write your code here ===\n",
    "        \n",
    "        super(LSTM_ASR_Discrete, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        self.lin1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, int(hidden_dim/2), batch_first=True, bidirectional=True)#, num_layers=2, dropout=0.4)\n",
    "\n",
    "        self.decoder = nn.Linear(hidden_dim, alphabet_size)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, batch_features):\n",
    "        \"\"\"\n",
    "        :param batch_features: batched acoustic features\n",
    "        :return: the output of your model (e.g., log probability)\n",
    "        \"\"\"\n",
    "        # === write your code here ===\n",
    "        \n",
    "        embeds = self.word_embeddings(x)\n",
    "\n",
    "        out = self.relu(self.lin1(embeds))\n",
    "\n",
    "        out = torch.nn.utils.rnn.pack_padded_sequence(out, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, _ = self.lstm(out)\n",
    "        lstm_out, _ = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        letter_probs = self.decoder(lstm_out)\n",
    "        log_probs = F.log_softmax(letter_probs, dim=2)\n",
    "\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eb3f73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--stage STAGE] [--features {Discrete,MFCC}]\n",
      "                             [--max-epochs MAX_EPOCHS] [--lr LR]\n",
      "                             [--custom-ctc]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/whhqund/Library/Jupyter/runtime/kernel-ec5c60bf-4276-4ca4-84f3-4be12947280e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# 2022 Dongji Gao\n",
    "# 2022 Yiwen Shao\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from dataset import AsrDataset\n",
    "from model import LSTM_ASR\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This function will be passed to your dataloader.\n",
    "    It pads word_spelling (and features) in the same batch to have equal length.with 0.\n",
    "    :param batch: batch of input samples\n",
    "    :return: (recommended) padded_word_spellings, \n",
    "                           padded_features,\n",
    "                           list_of_unpadded_word_spelling_length (for CTCLoss),\n",
    "                           list_of_unpadded_feature_length (for CTCLoss)\n",
    "    \"\"\"\n",
    "    # === write your code here ===\n",
    "    pass\n",
    "\n",
    "\n",
    "def train(train_dataloader, model, ctc_loss, optimizer):\n",
    "    # === write your code here ===\n",
    "    pass\n",
    "\n",
    "\n",
    "def decode():\n",
    "    # === write your code here ===\n",
    "    pass\n",
    "\n",
    "def compute_accuracy():\n",
    "    # === write your code here ===\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    training_set = YOUR_TRAINING_SET\n",
    "    test_set = YOUR_TEST_SET\n",
    "\n",
    "    train_dataloader = TRAIN_DATALOADER\n",
    "    test_dataloader = TEST_DATALOADER\n",
    "\n",
    "    model = LSTM_ASR\n",
    "\n",
    "    # your can simply import ctc_loss from torch.nn\n",
    "    loss_function = CTC_LOSS_FUNCTION\n",
    "\n",
    "    # optimizer is provided\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "    # Training\n",
    "    num_epochs = YOUR_NUM_EPOCHS\n",
    "    for epoch in range(num_epochs):\n",
    "        train(train_dataloader, model, loss_function, optimizer)\n",
    "\n",
    "    # Testing (totally by yourself)\n",
    "    decode()\n",
    "\n",
    "    # Evaluate (totally by yourself)\n",
    "    compute_accuracy()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d9c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
